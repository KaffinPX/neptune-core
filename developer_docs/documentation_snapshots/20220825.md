# Neptune Core Overview
Neptune Core is a multi-threaded and asynchronous program using the [tokio](https://tokio.rs/tokio/tutorial) framework for concurrent primitives. It connects to other clients through TCP/IP and accepts calls to its RPC server through HTTP/JSON.  Development also includes an RPC client that issues commands parsed from its command-line interface.

## Threads of the Neptune Core binary
There are four classes of threads:
- `main`: handles init and `main_loop`
- `peer[]`: handles `connect_to_peers` and `peer_loop`
- `mining`: runs `miner_loop`
- `rpc_server[]`: handles `rpc_server` for incoming RPC requests

## Threads of the RPC client binary
This is a separate program all together with a separate address space. This means the `state` object (see further down) is not available, and all data from Neptune Core must be received via RPC.
It only has one class of threads:
- `rpc_client[]`: handles `rpc_client` for parsing user-supplied command-line arguments and transforms them into outgoing RPC requests.

## Channels
The threads can communicate with each other through channels provided by the tokio framework. All communication goes through the main thread. There is e.g. no way for the miner to communicate with peer threads.

The channels are:
- peer to main: `mpsc`, "multiple producer, single consumer".
- main to peer: `broadcast`, messages can only be sent to *all* peer threads. If you only want one peer thread to act, the message must include an IP that represents the peer for which the action is intended.

## Global State
All threads have access to the global state and they can all read from it. Each type of thread can have its own local state that is not shared across thread, this is **not** what is discussed here.

The global state has three fields and they each follow some rules:
- `cli` CLI arguments. The state carries around the CLI arguments. These are read-only.
- `chain` Blockchain state. Consists of `lightState`, ephemeral, and `achivalState`, persistent. Only `main` thread may write to this. Archival state is stored both in a database and on disk.
- `network`, network state. Consists of `peer_map` for storing in memory info about all connected peers and `peer_databases` for storing info about banned peers. Both of these can be written to by main or by peer threads. `network` also contains a `syncing` value (only `main` may write) and `instance_id` which is read-only.

## Functionalities
- Peer discovery, state is managed by `main`
- Synchronization, state is managed by `main`

## Design Philosophies
- Avoid state through instruction pointer. This means that a request/response exchange should be handled without nesting of e.g matched messages from another peer. So when a peer thread requests a block from another peer it must return to the instruction pointer where it can receive *any* message from the peer and not only work if it actually gets the block as the next message. The reasoning behind this is that a peer thread must be able to respond to e.g. a peer discovery request message from the same peer before that peer responds with the requested block.

## Central Primitives
From `tokio`
- `spawn`
- `select!`
- `tokio::sync::Mutex`

From Std lib:
- `Arc`

## Persistent Memory
We use `rusty-leveldb` for our database layer with a custom-wrapper that makes it more type safe. `rusty-leveldb` allows for atomic writes within *one* database which is equivalent to a table in SQL lingo. So if you want atomic writes across multiple datatypes (you do want this!) you need to put that `enum` into the database and then cast the output type to the correct type. I think this is a low price to pay to achieve atomicity on the DB-layer.

Blocks are stored on disk and their position on disk is stored in the `block_index` database. Blocks are read from and written to disk using `mmap`.

## Challenges
- Deadlocks. Solution: always acquire locks in the same order. Note though that locks from `std::sync` may not be held over an `await`. The linter should tell you if you do this.
- We also have a few race conditions in the code base but we should be able to find these by running `run-multiple-instances-advanced.sh` that spins up eight nodes that eventually form a complete graph through peer discovery.

## Tracing
A structured way of inspecting a program when designing the RPC API, is to use tracing, which is a logger, that is suitable for programs with asynchronous control flow.
1. Get a feeling for the [core concepts](https://docs.rs/tracing/latest/tracing/).
2. Read tokio's [short tutorial](https://tokio.rs/tokio/topics/tracing).
3. View the [3 different formatters](https://docs.rs/tracing-subscriber/0.2.19/tracing_subscriber/fmt/index.html#formatters).
4. See what we can have eventually: https://tokio.rs/tokio/topics/tracing-next-steps

The main value-proposition of tracing is that you can add `#[instrument]` attribute over the function you currently work on. This will print the nested `trace!("")` statements. You can also do it more advanced:

```rust
#[instrument(ret, skip_all, fields(particular_arg = inputarg1*2), level="debug")]
fn my_func(&self, inputarg1: u32, inputarg2: u32) -> u32 {
  debug!("This will be visible from `stdout`");
  info!("This prints");
  trace!("This does not print {:#?}", inputarg2);
  inputarg1 * 42 + inputarg2
}
```

Prints the return value, but none of the args (default behaviour is to prints all arguments with std::fmt::Debug formatter). It creates a new key with a value that is the double of the `inputarg1` and prints that.
It then prints everything that is `debug` level or above, where `trace < debug < info < warn < error`, so here the `trace!()` is omitted.  You configure the lowest level you want to see with environment variable `RUST_LOG=debug`.

## RPC
To develop a new RPC, it can be productive to view two terminals simultaneously and run one of the following commands in each:

```bash
RUST_LOG=debug cargo run -- --mine --network regtest # Window1 RPC-server
RUST_LOG=trace cargo run --bin rpc_client -- --server-addr 127.0.0.1:9799 send '[{"recipient_address": "0399bb06fa556962201e1647a7c5b231af6ff6dd6d1c1a8599309caa126526422e", "amount": 11}]' # Window2 RPC-client
```

Note that the client exists quickly, so here the `.pretty()` tracing subscriber is suitable, while `.compact()` is perhaps better for the server.

